# Week 02 Checkpoint

Class: CS631

Subject: # Advanced Programming in the UNIX Environment

Date: 2025-09-14

Teacher: Jan Schaumann

1. What happens if you run `ulimit -n 0`? Why?
	- 'ulimit' with the -n flag will limit the maximum number of open file descriptors. When I run 'ulimit -n 0' on my NetBSD VM, it will set the maximum number of open file descriptors to 0, meaning processes will no longer be able to open any files. Attempts to open, create, or write to any file will fail with an "Too many open files" error. This ultimately breaks my current VM instance, as I can no longer run commands like 'vim', so I can't even view and edit the content of files. Whenever I input a character into the shell, I get several errors telling me that 'pipe failed' and 'too many open files'. Meaning that not only can't I view or edit files, but now NetBSD tells me 'too many open files', so effectively, nothing that utilizes files can be run anymore, ultimately crashing my VM, and I needed to restart it. This is all because 'ulimit -n 0' will set the maximum number of files that can be opened (i.e file descriptors available) to 0, so programs that require access or perform actions in files will crash.
2. Complete the file descriptors exercise. What, if anything, did you learn? Any other comments?
	- I learned quite a lot from this exercise. The file descriptors for stdin, stdout, and stderr are 0, 1, and 2 respectively, the constants for STDIN_FILENO, STDOUT_FILENO, and STDERR_FILENO are also 0, 1, and 2 respectively, meaning that the file descriptors for each stream are the same as the constant set for it. So stdin corresponds with STDIN_FILENO, stdout corresponds with STDOUT_FILENO, and stderr corresponds with STDERR_FILENO. The open(2) function will return the file descriptor number if it is successful, and return a negative one for failure. fopen(3) on the other hand will return a FILE* on success and NULL if it fails, and to get the file descriptor number for that FILE* you need to use fileno() function for it. When I open the same file twice in a row, the file descriptors will be different (3 for the first open, 4 for the second), if I instead open and then close the file, and then open it a second time, the file descriptors printed will be the same (3, and then after closing, 3 again). I then opened the same file twice, so there is two different file descriptors, for the first file descriptor (3) I write "Hello world" and the second file descriptor I write "foo bar", after the second write, I notice "Hello world" has been overwritten with "foo bar" meaning that each file descriptor has independent offsets. When performing alternating read(2) and write(2) system calls on a single file descriptor, the current file position is advanced by the number of bytes read or written. To replace a specific word in a file is somewhat difficult, and I need to use lseek(2) in order to seek back and set the position to the beginning of the word I want to replace, and then perform a write(2) to replace that word.
3. Identify a flag that may be passed to open(2) that was _not_ discussed in the videos or slides.  What does it do? Is it OS specific?
	- One flag that was not discussed in the videos of slides is O_EXLOCK. I found the flag when looking through the man pages of write(2) on my MacBook M1 Pro running macOS Sequoia version 15.6.1. When the flag is set, it will atomically obtain an exclusive lock to the file. The O_EXLOCK is available on macOS and other BSD-based systems, but not on Linux.
4. Run the hole.c program on different Unix systems, does the behavior differ? why?
	- I ran hole.c on my NetBSD VM, my native macOS machine, and a Ubuntu 22.04 virtual machine that each have 64GB of space (except my native machine). Yes, the behavior is different, it’s the filesystem that causes the difference. On my NetBSD VM, it only takes 96 blocks, on my macOS it takes all 20000 blocks, and on Linux (Ubuntu) it takes up only 8 blocks. This is because on filesystems that support sparse files, doing lseek() past EOF and then write() creates a hole, the file’s logical size is large, but it consumes few disk blocks. On filesystems without sparse-file support, the hole is materialized as real zero bytes, so blocks used is approximately the file size. Basically one can seek past the end of a file and thereby create a sparse file. But the file system used by the operating system needs to support it, or else one just gets a file with NULL bytes written to disk. If the filesystem supports sparse files, then the kernel will supply NULL bytes when you read the hole, but they will not actually be on disk. 
5. Write 10 bytes to file, then do lseek to end of file, what happens to the offset?
	- It will land back to offset 10. After a write, the file offset advances by the number of bytes written, and the file’s size becomes 10. An lseek(fd, 0, SEEK_END) sets the offset to the current file size, so for a file with only those 10 bytes, EOF is at 10. 
6. What is the difference between cmd >file 2>&1 and 2>&1 >file
	- Yes. In cmd >file 2>&1, stdout is sent to file and then stderr is duplicated to stdout (which now points to file), so both end up in the file. In cmd 2>&1 >file, stderr is first duplicated to the current stdout (the terminal), and only afterward stdout is redirected to file, so stdout goes to the file while stderr stays on the terminal. 